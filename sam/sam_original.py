from utils import *
import os
import cv2
import shutil
import argparse
# if using Apple MPS, fall back to CPU for unsupported ops
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image
import pandas as pd
import random
import hydra
from hydra import compose
from hydra.core.global_hydra import GlobalHydra
from omegaconf import OmegaConf
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
'''
Use this command to run:
1 stands for single point prompt, and 2 stands for single box prompt:
    python /root/vmcbh/sam/sam_original.py 1 /root/Dataset/BUS_reduced/images /root/Dataset/BUS_reduced/labels /root/Dataset/BUS_reduced/results
-is_demo = False will run the whole dataset, otherwise just 10 images:
    python /root/vmcbh/sam/sam_original.py 1 /root/Dataset/BUS_reduced/images /root/Dataset/BUS_reduced/labels /root/Dataset/BUS_reduced/results --is_demo False
'''
def sam_original(prompt_type, img_folder, prompt_folder, output_folder, img_num=10, is_demo=True, file_extension=".png"):
    """
    Use the ORIGINAL SAM MODEL to segment and evaluate a dataset.

    Args:
        prompt_type (num): 1 stands for single point prompt, 2 stands for single box prompt
        img_folder (str): Path to the orginal images (Ultrasound image) folder.
        prompt_folder (str): Path to the masks (ground truth or generated by U-Net) folder.
        result_folder (str): Path to the folder of output images and metrics.
        batch_size (num): 
        is_demo (bool): True means this run is for testing or presenting, will just run a part of the dataset (10 images)
        file_extension (str): File extension to filter and match (default: ".png").
    """
    prompt_type = int(prompt_type)
    img_num = int(img_num)

    # Ensure the target folder exists
    os.makedirs(output_folder, exist_ok=True)

    # Check the computing device
    # select the device for computation
    if torch.cuda.is_available():
        device = torch.device("cuda")
        current_device = torch.cuda.current_device()
        print("Current GPU device:", torch.cuda.get_device_name(current_device))
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
    else:
        device = torch.device("cpu")
    print(f"using device: {device}")

    if device.type == "cuda":
        # use bfloat16 for the entire notebook
        torch.autocast("cuda", dtype=torch.bfloat16).__enter__()
        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)
        if torch.cuda.get_device_properties(0).major >= 8:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
    elif device.type == "mps":
        print(
            "\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might "
            "give numerically different outputs and sometimes degraded performance on MPS. "
            "See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion."
        )
    
    sam2_checkpoint = "/root/checkpoints/sam2_hiera_large.pt"
    model_cfg = "sam2_hiera_l.yaml"
    sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)
    predictor = SAM2ImagePredictor(sam2_model)

    print(f"image folder: {img_folder}")
    print(f"prompt folder: {prompt_folder}")
    print(f"output folder: {output_folder}")

    
    max_count = img_num
    count = 0
    # suffix = "_pred"
    filelist = os.listdir(img_folder)
    sorted_filelist = sorted(filelist)  # from a to z
    for filename in sorted_filelist:
        if filename.endswith(file_extension):  # only operate image file
            img_path = os.path.join(img_folder, filename)
            # type = os.path.splitext(filename)[1]
            # prompt_filename = os.path.splitext(filename)[0] + suffix + type
            prompt_path = os.path.join(prompt_folder, filename)
            prompt = Image.open(prompt_path)
            prompt = np.array(prompt.convert("L")) / 255  # convert to 01 img
            '''generate prompt'''
            if prompt_type == 1:
                median_x, median_y = find_median_point(prompt)
                prompt_input = np.array([[median_y, median_x]])  # note that is (y,x)
            elif prompt_type == 2:
                prompt_input = find_rect_box(prompt)
            '''generate prompt'''

            # The shape of mask is [h,w,4], including a transparent channel
            # The shape of mask_binary is [h,w]. It's a 0-1 binary array
            mask_binary, mask = segment(prompt_type, prompt_input, img_path, predictor, show_mask=False)  # 1 stands for Single Point Prompt Type
            output_path = os.path.join(output_folder,filename)

            color = np.array([30/255, 144/255, 255/255, 0.6])
            h, w = mask_binary.shape[-2:]
            mask = mask.astype(np.uint8)
            mask = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
            mask_binary = mask_binary.astype(np.uint8)
            mask_binary = mask_binary.reshape(h, w)

            mask_binary_img = mask_binary * 255
            mask_binary_img = Image.fromarray(mask_binary_img)
            mask_binary_img.save(output_path)
            #I added a break line to run just one single image, delete it to run all images in the folder
            if is_demo == True:
                count = count + 1
                if count >= max_count:
                    break


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Use the ORIGINAL SAM MODEL to segment and evaluate a dataset.")
    parser.add_argument("prompt_type", help="1 stands for single point prompt, 2 stands for single box prompt")
    parser.add_argument("img_folder", help="Path to the orginal images (Ultrasound image) folder.")
    parser.add_argument("prompt_folder", help="Path to the masks (ground truth or generated by U-Net) folder.")
    parser.add_argument("output_folder", help="Path to the output folder where matched files will be copied.")
    parser.add_argument("--img_num", default="10", help="Number or the batch size")
    parser.add_argument("--is_demo", default=True, help="True means this run is for testing or presenting, will just run a part of the dataset (10 images)")
    parser.add_argument("--extension", default=".png", help="File extension to match (default: '.png').")

    args = parser.parse_args()

    sam_original(args.prompt_type, args.img_folder, args.prompt_folder, args.output_folder, args.img_num, args.is_demo, args.extension)
