{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7Y5yXh7uKF1",
        "outputId": "b64b35cc-5dd5-4f20-9d4b-30ada2e6a634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# import drive files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 原始 Excel 文件路径\n",
        "input_path = \"/content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/BIRADS&FOLD.xlsx\"\n",
        "\n",
        "# 读取 Excel 文件\n",
        "df = pd.read_excel(input_path)\n",
        "\n",
        "# 重命名列名\n",
        "df.rename(columns={\"BIRADS-reader1\": \"type\"}, inplace=True)\n",
        "\n",
        "# 新 CSV 文件保存路径\n",
        "output_path = \"/content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/BIRADS_FOLD_renamed.csv\"\n",
        "\n",
        "# 保存为 CSV 格式\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"✅ 文件已成功保存为 CSV 格式，路径为：\", output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiBvjxtuuWfY",
        "outputId": "393e11a4-a481-4845-d4f1-46894e4ca57b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 文件已成功保存为 CSV 格式，路径为： /content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/BIRADS_FOLD_renamed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "birads转换"
      ],
      "metadata": {
        "id": "kyFELmvhA2lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 原始 CSV 文件路径\n",
        "input_path = \"/content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/BIRADS_FOLD_renamed.csv\"\n",
        "\n",
        "# 读取 CSV 文件\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# 将 \"type\" 列统一转换为小写字符串并去除空格\n",
        "df['type'] = df['type'].astype(str).str.lower().str.strip()\n",
        "\n",
        "# 定义四分类映射规则\n",
        "mapping = {\n",
        "    \"1\": 0,   # Normal\n",
        "    \"2\": 1,   # Benign\n",
        "    \"3\": 1,   # Benign\n",
        "    \"4\": 2,   # Suspicious\n",
        "    \"4a\": 2,  # Suspicious\n",
        "    \"4b\": 2,  # Suspicious\n",
        "    \"4c\": 2,  # Suspicious\n",
        "    \"5\": 3    # Malignant\n",
        "}\n",
        "\n",
        "# 映射并创建新列\n",
        "df['label'] = df['type'].map(mapping)\n",
        "\n",
        "# 转换为整数类型\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# 删除原始 type 列并将 label 重命名为 type\n",
        "df.drop(columns=['type'], inplace=True)\n",
        "df.rename(columns={'label': 'type'}, inplace=True)\n",
        "\n",
        "# 保存新的 CSV 文件\n",
        "output_path = \"/content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/BIRADS_FOLD_4class.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"✅ 四分类处理完成，新文件已保存到：\", output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Woj3LOQJA6Yu",
        "outputId": "042f5cf6-1976-4f59-a93c-84a517c7b715"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 四分类处理完成，新文件已保存到： /content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/BIRADS_FOLD_4class.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Import"
      ],
      "metadata": {
        "id": "8XD730dcLrXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "from skimage import io, color\n",
        "from skimage import color as skcolor\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as F\n",
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "metK2W9aLrwD",
        "outputId": "fb62d33d-515d-429a-bbac-568e9260da95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/101.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 折交叉验证代码"
      ],
      "metadata": {
        "id": "qCLY4GjLLd80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*DBTDATA*旧的"
      ],
      "metadata": {
        "id": "D7RWtAndPO1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet50_5fold_training.py\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# --- Dataset class with 5-fold support ---\n",
        "class DBTData(Dataset):\n",
        "    def __init__(self, csv_path, image_root, fold=0, mode='train', crop_to=(224, 224), resize_to=(256, 256), color=True):\n",
        "        self._resize_to = resize_to\n",
        "        self._color = color\n",
        "        self.mode = mode\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df[\"label\"] = df[\"type\"]  # 使用已有四分类标签列\n",
        "\n",
        "        if mode == \"train\":\n",
        "            df = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
        "        elif mode == \"valid\":\n",
        "            df = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
        "\n",
        "        self.df = df\n",
        "        self.image_root = image_root\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "\n",
        "        for i in range(len(df)):\n",
        "            img_id = df.at[i, \"ID\"]\n",
        "            label = df.at[i, \"label\"]\n",
        "            image_path = os.path.join(image_root, img_id + \".png\")\n",
        "            if not os.path.exists(image_path):\n",
        "                image_path = os.path.join(image_root, img_id + \".jpg\")\n",
        "\n",
        "            if os.path.exists(image_path):\n",
        "                img = mpimg.imread(image_path)\n",
        "                if len(img.shape) == 3:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                self.x.append(img)\n",
        "                self.y.append(label)\n",
        "\n",
        "        self.y = torch.tensor(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[idx]\n",
        "        y = self.y[idx]\n",
        "        if self._color:\n",
        "            x = np.stack([x] * 3, axis=-1)  # Grayscale to RGB\n",
        "\n",
        "        x = Image.fromarray(x.astype(np.uint8))\n",
        "\n",
        "        transform = T.Compose([\n",
        "            T.Resize(self._resize_to),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "        x = transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "# --- Training and validation ---\n",
        "def train_fold(fold, csv_path, image_root, num_classes=4, batch_size=32, num_workers=0, device='cuda'):\n",
        "    print(f\"\\n======== Fold {fold} ========\")\n",
        "\n",
        "    train_set = DBTData(csv_path, image_root, fold=fold, mode='train')\n",
        "    valid_set = DBTData(csv_path, image_root, fold=fold, mode='valid')\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-8)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    writer = SummaryWriter(log_dir=f'./runs/fold{fold}')\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        train_loss, train_acc = [], []\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "            train_acc.append((y_pred.argmax(dim=1) == y).float().mean().item())\n",
        "\n",
        "        val_loss, val_acc = [], []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in valid_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = criterion(y_pred, y)\n",
        "                val_loss.append(loss.item())\n",
        "                val_acc.append((y_pred.argmax(dim=1) == y).float().mean().item())\n",
        "\n",
        "        avg_train_loss = np.mean(train_loss)\n",
        "        avg_val_loss = np.mean(val_loss)\n",
        "        avg_train_acc = np.mean(train_acc)\n",
        "        avg_val_acc = np.mean(val_acc)\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train Loss {avg_train_loss:.4f}, Acc {avg_train_acc:.4f} | Val Loss {avg_val_loss:.4f}, Acc {avg_val_acc:.4f}\")\n",
        "\n",
        "        writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
        "        writer.add_scalar('Loss/valid', avg_val_loss, epoch)\n",
        "        writer.add_scalar('Acc/train', avg_train_acc, epoch)\n",
        "        writer.add_scalar('Acc/valid', avg_val_acc, epoch)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), f\"best_model_fold{fold}.pth\")\n",
        "\n",
        "# --- Entry Point ---\n",
        "if __name__ == '__main__':\n",
        "    csv_path = \"/content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/BIRADS_FOLD_4class.csv\"\n",
        "    image_root = \"/content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/data\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    for fold in range(5):\n",
        "        train_fold(fold, csv_path, image_root, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "Hr2wqq1PPSe5",
        "outputId": "02c91621-a15e-4075-e5f4-ed953c38d3c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Fold 0 ========\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c5429b0d6c98>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mtrain_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-c5429b0d6c98>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(fold, csv_path, image_root, num_classes, batch_size, num_workers, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n======== Fold {fold} ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBTData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBTData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c5429b0d6c98>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_path, image_root, fold, mode, crop_to, resize_to, color)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0;34m\"``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             )\n\u001b[0;32m-> 1502\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1504\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             except (\n\u001b[1;32m    149\u001b[0m                 \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# end of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_accept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"not a PNG file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "utils.py"
      ],
      "metadata": {
        "id": "PPSqhn6vXS6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.transforms import CenterCrop, Grayscale\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/PCC2/HoVerTrans-main/HoVerTrans-main\")\n",
        "\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0.0, variance=1.0, amplitude=1.0, p=1):\n",
        "        self.mean = mean\n",
        "        self.variance = variance\n",
        "        self.amplitude = amplitude\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.uniform(0, 1) < self.p:\n",
        "            img = np.array(img)\n",
        "            h, w = img.shape\n",
        "            N = self.amplitude * np.random.normal(loc=self.mean, scale=self.variance, size=(h, w))\n",
        "            img = N + img\n",
        "            img[img > 255] = 255\n",
        "            img = Image.fromarray(img.astype('uint8')).convert('L')\n",
        "            return img\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "class AddBlur(object):\n",
        "    def __init__(self, kernel=3, p=1):\n",
        "        self.kernel = kernel\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.uniform(0, 1) < self.p:\n",
        "            img = np.array(img)\n",
        "            img = cv2.blur(img, (self.kernel, self.kernel))\n",
        "            img = Image.fromarray(img.astype('uint8')).convert('L')\n",
        "            return img\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "class BIRADSDataset(Dataset):\n",
        "    def __init__(self, root, transform, csv_path):\n",
        "        super().__init__()\n",
        "        self.data_root = root\n",
        "        self.transform = transform\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "\n",
        "        # 自动补充扩展名\n",
        "        base_name = row['ID']\n",
        "        for ext in ['.jpg', '.png', '.jpeg']:\n",
        "            file_path = os.path.join(self.data_root, base_name + ext)\n",
        "            if os.path.exists(file_path):\n",
        "                break\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"{base_name} with any known extension not found\")\n",
        "\n",
        "        label = int(row['type'])  # 根据你的CSV列名可能是 'type'\n",
        "\n",
        "        img = Image.open(file_path)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return {'imgs': img, 'labels': label, 'names': base_name}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def get_dataset(imgpath, csvpath, img_size, mode='train'):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.Grayscale(),\n",
        "        transforms.CenterCrop((img_size, img_size)),\n",
        "        AddGaussianNoise(amplitude=random.uniform(0, 1), p=0.5),\n",
        "        AddBlur(kernel=3, p=0.5),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=(0.5, 2), contrast=(0.5, 2)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.Grayscale(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    transform = train_transform if mode == 'train' else test_transform\n",
        "    dataset = BIRADSDataset(imgpath, transform, csvpath)\n",
        "    return dataset\n",
        "\n",
        "def confusion_matrix(preds, labels, conf_matrix):\n",
        "    preds = torch.flatten(preds)\n",
        "    labels = torch.flatten(labels)\n",
        "    for p, t in zip(preds, labels):\n",
        "        conf_matrix[int(p), int(t)] += torch.tensor(1)\n",
        "    return conf_matrix\n"
      ],
      "metadata": {
        "id": "Kz11DUuSXWn9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "config.py"
      ],
      "metadata": {
        "id": "kHfVARSCXeSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "修改"
      ],
      "metadata": {
        "id": "WSOeaalMdsFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/PCC2/HoVerTrans-main/HoVerTrans-main\")\n",
        "def config(args=None):  # ✅ 添加 args=None\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Paths\n",
        "    parser.add_argument('--data_path', type=str, required=True, help='Directory containing image files')\n",
        "    parser.add_argument('--csv_path', type=str, required=True, help='CSV file path with ID and type columns')\n",
        "\n",
        "    # Output\n",
        "    parser.add_argument('--model_name', type=str, default='hovertrans')\n",
        "    parser.add_argument('--model_path', type=str, default='./weight')\n",
        "    parser.add_argument('--writer_comment', type=str, default='BIRADS4CLASS')\n",
        "    parser.add_argument('--save_model', type=bool, default=True)\n",
        "\n",
        "    # Basic training config\n",
        "    parser.add_argument('--img_size', type=int, default=256)\n",
        "    parser.add_argument('--batch_size', type=int, default=32)\n",
        "    parser.add_argument('--class_num', type=int, default=4)  # Four-class classification\n",
        "    parser.add_argument('--fold', type=int, default=5)\n",
        "    parser.add_argument('--epochs', type=int, default=150)\n",
        "    parser.add_argument('--log_step', type=int, default=5)\n",
        "    parser.add_argument('--lr', type=float, default=0.0001)\n",
        "\n",
        "    # Model architecture (HoVer-Transformer)\n",
        "    parser.add_argument('--patch_size', type=list, default=[2, 2, 2, 2])\n",
        "    parser.add_argument('--hover_size', type=list, default=[2, 2, 2, 2])\n",
        "    parser.add_argument('--dim', type=list, default=[4, 8, 16, 32])\n",
        "    parser.add_argument('--depth', type=list, default=[2, 4, 4, 2])\n",
        "    parser.add_argument('--num_heads', type=list, default=[2, 4, 8, 16])\n",
        "    parser.add_argument('--num_inner_head', type=list, default=[2, 4, 8, 16])\n",
        "\n",
        "    # Optimization setup\n",
        "    parser.add_argument('--loss_function', type=str, default='CE')\n",
        "    parser.add_argument('--optimizer', type=str, default='AdamW', choices=['SGD', 'Adam', 'AdamW'])\n",
        "    parser.add_argument('--scheduler', type=str, default='cosine', choices=['cosine', 'step'])\n",
        "    parser.add_argument('--warmup_epochs', type=int, default=10)\n",
        "    parser.add_argument('--warmup_decay', type=float, default=0.01)\n",
        "    parser.add_argument('--min_lr', type=float, default=1e-6)\n",
        "    parser.add_argument('--step', type=int, default=5)\n",
        "\n",
        "    return parser.parse_args(args)  # ✅ 用 args 来解析传入的参数\n"
      ],
      "metadata": {
        "id": "CssttTBRdtOt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hovertrans.py"
      ],
      "metadata": {
        "id": "3CjPQzY3X-OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import trunc_normal_\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/PCC2/HoVerTrans-main/HoVerTrans-main\")\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qk = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop, inplace=False)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop, inplace=True)\n",
        "\n",
        "    def forward(self, x, relative_pos=None):\n",
        "        B, N, C = x.shape\n",
        "        qk = self.qk(x).reshape(B, N, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k = qk.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
        "        v = self.v(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        if relative_pos is not None:\n",
        "            attn += relative_pos\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        drop_probs = (drop, drop)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop_probs[0])\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop2 = nn.Dropout(drop_probs[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "class Merge(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, patch_size, in_chans=3):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_dim*2, in_dim*4, kernel_size=1, stride=1),\n",
        "            nn.BatchNorm2d(in_dim*4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_dim*4, in_dim*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_dim*4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_dim*4, out_dim, kernel_size=1, stride=1),\n",
        "            nn.BatchNorm2d(out_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            )\n",
        "        self.in_dim = in_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.norm_in = nn.LayerNorm(in_dim)\n",
        "        self.norm_out = nn.LayerNorm(out_dim)\n",
        "\n",
        "    def forward(self, pixel_embed1, pixel_embed2):\n",
        "        H_p = W_p = self.patch_size\n",
        "        W_column = pixel_embed1.size(1)\n",
        "        BW_column, H_row, _ = pixel_embed2.size()\n",
        "        B = BW_column // W_column\n",
        "        assert H_row == W_column\n",
        "\n",
        "        img1 = pixel_embed1.reshape(B, H_row, W_column, H_p, W_p, self.in_dim).permute(0, 5, 1, 3, 2, 4).reshape(B, self.in_dim, H_row*H_p, W_column*W_p)\n",
        "        img2 = pixel_embed2.reshape(B, H_row, W_column, H_p, W_p, self.in_dim).permute(0, 5, 1, 3, 2, 4).reshape(B, self.in_dim, H_row*H_p, W_column*W_p)\n",
        "        img_reshaped = torch.cat([img1, img2], dim=1)\n",
        "        img_merge = self.conv(img_reshaped)\n",
        "\n",
        "        return img_merge\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, words_in_sentence, patch_size, sentences, in_chans=3, num_heads=2, num_inner_heads=4, mlp_ratio=4.,\n",
        "            qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        # Inner transformer\n",
        "        self.patch_size = patch_size\n",
        "        words = patch_size * patch_size\n",
        "        self.norm_in = norm_layer(dim*words)\n",
        "        self.attn_in1 = Attention(\n",
        "            dim*words, dim*words, num_heads=num_inner_heads, qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.attn_in2 = Attention(\n",
        "            dim*words, dim*words, num_heads=num_inner_heads, qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm_mlp_in = norm_layer(dim*words)\n",
        "        self.mlp_in1 = Mlp(in_features=dim*words, hidden_features=int(dim*words * 4),\n",
        "            out_features=dim*words, act_layer=act_layer, drop=drop)\n",
        "        self.mlp_in2 = Mlp(in_features=dim*words, hidden_features=int(dim*words * 4),\n",
        "            out_features=dim*words, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        self.norm_proj = norm_layer(dim*words)\n",
        "        self.proj1 = nn.Linear(dim*words, dim*words, bias=True)\n",
        "        self.proj2 = nn.Linear(dim*words, dim*words, bias=True)\n",
        "\n",
        "        # Outer transformer\n",
        "        self.norm_out = norm_layer(dim * words_in_sentence)\n",
        "        self.attn_out1 = Attention(\n",
        "            dim * words_in_sentence, dim * words_in_sentence, num_heads=num_heads, qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.attn_out2 = Attention(\n",
        "            dim * words_in_sentence, dim * words_in_sentence, num_heads=num_heads, qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm_mlp = norm_layer(dim * words_in_sentence)\n",
        "        self.mlp1 = Mlp(in_features=dim * words_in_sentence, hidden_features=int(dim * words_in_sentence * mlp_ratio),\n",
        "            out_features=dim * words_in_sentence, act_layer=act_layer, drop=drop)\n",
        "        self.mlp2 = Mlp(in_features=dim * words_in_sentence, hidden_features=int(dim * words_in_sentence * mlp_ratio),\n",
        "            out_features=dim * words_in_sentence, act_layer=act_layer, drop=drop)\n",
        "        # self.relative_pos1 = nn.Parameter(torch.randn(1, num_heads, sentences, sentences))\n",
        "        # self.relative_pos2 = nn.Parameter(torch.randn(1, num_heads, sentences, sentences))\n",
        "\n",
        "    def forward(self, pixel_embed1, pixel_embed2, row_embed, column_embed, relative_pos=None):\n",
        "        _, W_grid, _ = pixel_embed1.size()\n",
        "        H_grid = W_grid\n",
        "        H_p = W_p = self.patch_size\n",
        "        B, N, C = row_embed.size()\n",
        "\n",
        "        # outer\n",
        "        assert N == H_grid\n",
        "        row_embed = row_embed + self.drop_path(self.attn_out1(self.norm_out(row_embed)))\n",
        "        row_embed = row_embed + self.drop_path(self.mlp1(self.norm_mlp(row_embed)))\n",
        "\n",
        "        assert N == W_grid\n",
        "        column_embed = column_embed + self.drop_path(self.attn_out2(self.norm_out(column_embed)))\n",
        "        column_embed = column_embed + self.drop_path(self.mlp2(self.norm_mlp(column_embed)))\n",
        "\n",
        "        # inner\n",
        "        pixel_embed1 = pixel_embed1 + self.proj1(self.norm_proj(row_embed.reshape(B*H_grid, H_p, W_grid, W_p, -1).transpose(1, 2).reshape(B*H_grid, W_grid, -1)))\n",
        "        attn_patch1 = self.attn_in1(self.norm_in(pixel_embed1.reshape(B, H_grid*W_grid, -1)))\n",
        "        pixel_embed1 = pixel_embed1 + self.drop_path(attn_patch1.reshape(B*H_grid, W_grid, -1))\n",
        "        pixel_embed1 = pixel_embed1 + self.proj2(self.norm_proj(column_embed.reshape(B, W_grid, H_grid, -1).transpose(1, 2).reshape(B*H_grid, W_grid, -1)))\n",
        "        attn_patch2 = self.attn_in2(self.norm_in(pixel_embed1.reshape(B, H_grid*W_grid, -1)))\n",
        "        pixel_embed1 = pixel_embed1 + self.drop_path(attn_patch2.reshape(B*H_grid, W_grid, -1))\n",
        "        pixel_embed1 = pixel_embed1 + self.drop_path(self.mlp_in1(self.norm_mlp_in(pixel_embed1)))\n",
        "\n",
        "        pixel_embed2 = pixel_embed2 + self.proj2(self.norm_proj(column_embed.reshape(B, W_grid, H_grid, -1).transpose(1, 2).reshape(B*H_grid, W_grid, -1)))\n",
        "        attn_patch3 = self.attn_in2(self.norm_in(pixel_embed2.reshape(B, H_grid*W_grid, -1)))\n",
        "        pixel_embed2 = pixel_embed2 + self.drop_path(attn_patch3.reshape(B*H_grid, W_grid, -1))\n",
        "        pixel_embed2 = pixel_embed2 + self.proj1(self.norm_proj(row_embed.reshape(B*H_grid, H_p, W_grid, W_p, -1).transpose(1, 2).reshape(B*H_grid, W_grid, -1)))\n",
        "        attn_patch4 = self.attn_in1(self.norm_in(pixel_embed2.reshape(B, H_grid*W_grid, -1)))\n",
        "        pixel_embed2 = pixel_embed2 + self.drop_path(attn_patch4.reshape(B*H_grid, W_grid, -1))\n",
        "        pixel_embed2 = pixel_embed2 + self.drop_path(self.mlp_in2(self.norm_mlp_in(pixel_embed2)))\n",
        "\n",
        "        return pixel_embed1, pixel_embed2, row_embed, column_embed\n",
        "class ToEmbed(nn.Module):\n",
        "    def __init__(self, img_size=256, in_chans=3, patch_size=2, dim=8):\n",
        "        super().__init__()\n",
        "        img_size_tuple = (img_size, img_size)\n",
        "        row_patch_size = (patch_size, img_size)\n",
        "        self.grid_size = (img_size_tuple[0] // row_patch_size[0], img_size_tuple[1] // row_patch_size[1])\n",
        "        num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.patch_size = patch_size\n",
        "        self.img_size = img_size_tuple\n",
        "        self.num_patches = num_patches\n",
        "        self.row_patch_size = row_patch_size\n",
        "        self.dim = dim\n",
        "        row_pixel = row_patch_size[0] * row_patch_size[1]\n",
        "\n",
        "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm_proj = nn.LayerNorm(row_pixel * dim)\n",
        "        self.proj1 = nn.Linear(row_pixel * dim, row_pixel * dim)\n",
        "        self.proj2 = nn.Linear(row_pixel * dim, row_pixel * dim)\n",
        "\n",
        "    def forward(self, x, pixel_pos=None):\n",
        "        B, C, H, W = x.shape\n",
        "        assert H == self.img_size[0]\n",
        "        assert W == self.img_size[1]\n",
        "\n",
        "        x = self.unfold(x)\n",
        "        if pixel_pos is not None:\n",
        "            x = x + pixel_pos\n",
        "        x = x.transpose(1, 2).reshape(B , self.num_patches, self.num_patches, self.dim, self.patch_size, self.patch_size)\n",
        "\n",
        "        pixel_embed = x.permute(0, 1, 2, 4, 5, 3).reshape(B * self.num_patches, -1, self.patch_size*self.patch_size*self.dim)\n",
        "        row_embed = self.norm_proj(x.permute(0, 1, 4, 2, 5, 3).reshape(B, self.num_patches, -1))\n",
        "        column_embed =  self.norm_proj(x.permute(0, 2, 1, 4, 5, 3).reshape(B, self.num_patches, -1))\n",
        "\n",
        "        return pixel_embed, pixel_embed, row_embed, column_embed\n",
        "\n",
        "class Stage(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_chans, dim, out_dim, num_heads=2, num_inner_head=2, depth=1,\n",
        "                     mlp_ratio=4., qkv_bias=False, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                     norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pixel_embed = ToEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, dim=dim)\n",
        "        row_patch_size = self.pixel_embed.row_patch_size\n",
        "        self.row_pixel = row_patch_size[0] * row_patch_size[1]\n",
        "        self.patch_pixel = patch_size*patch_size\n",
        "        self.num_patches = self.pixel_embed.num_patches\n",
        "\n",
        "        blocks = []\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        for i in range(depth):\n",
        "            blocks.append(Block(\n",
        "                dim=dim, words_in_sentence=self.row_pixel, num_heads=num_heads, num_inner_heads=num_inner_head,\n",
        "                mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate, in_chans=in_chans,\n",
        "                drop_path=dpr[i], norm_layer=norm_layer, patch_size=patch_size, sentences=self.num_patches))\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.merge = Merge(in_dim=dim, out_dim=out_dim, patch_size=patch_size, in_chans=in_chans,)\n",
        "\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "    def forward(self, x, pixel_pos=None, row_pos=None, column_pos=None):\n",
        "        pixel_embed1, pixel_embed2, row_embed, column_embed = self.pixel_embed(x, pixel_pos)\n",
        "        if row_pos is not None:\n",
        "            row_embed = row_embed + row_pos\n",
        "        row_embed = self.pos_drop(row_embed)\n",
        "        if column_pos is not None:\n",
        "            column_embed = column_embed + column_pos\n",
        "        column_embed = self.pos_drop(column_embed)\n",
        "        for blk in self.blocks:\n",
        "            pixel_embed1, pixel_embed2, row_embed, column_embed = blk(pixel_embed1, pixel_embed2, row_embed, column_embed)\n",
        "        img_merge = self.merge(pixel_embed1, pixel_embed2)\n",
        "\n",
        "        return img_merge\n",
        "\n",
        "class HoverTrans(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=32, in_chans=3, num_classes=4, embed_dim=768, dim=48, depth=12,\n",
        "                 num_heads=12, num_inner_head=4, mlp_ratio=4., drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.1, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        stride = [4, 2, 2, 2]\n",
        "        self.stage = nn.ModuleList([])\n",
        "        self.downsample = nn.ModuleList([])\n",
        "        for i in range(4):\n",
        "            if i == 0:\n",
        "                self.stage.append(Stage(img_size=img_size//stride[i], patch_size=patch_size[i], in_chans=in_chans, dim=dim[i], out_dim=dim[i]*2,\n",
        "                            depth=depth[i], num_heads=num_heads[i], num_inner_head=num_inner_head[i], mlp_ratio=mlp_ratio, qkv_bias=False,\n",
        "                            drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate, norm_layer=nn.LayerNorm))\n",
        "                num_patches = self.stage[i].num_patches\n",
        "                row_pixel = self.stage[i].row_pixel\n",
        "                patch_pixel = self.stage[i].patch_pixel\n",
        "                self.row_pos = nn.Parameter(torch.zeros(1, num_patches, row_pixel * dim[i]))\n",
        "                self.column_pos = nn.Parameter(torch.zeros(1, num_patches, row_pixel * dim[i]))\n",
        "                self.pixel_pos = nn.Parameter(torch.zeros(1, dim[i]*patch_pixel, num_patches * num_patches))\n",
        "\n",
        "                self.downsample.append(nn.Sequential(\n",
        "                            nn.Conv2d(in_chans, in_chans*2, 3, stride=2, padding=1),\n",
        "                            nn.BatchNorm2d(in_chans*2),\n",
        "                            nn.ReLU(inplace=True),\n",
        "                            nn.Conv2d(in_chans*2, in_chans*4, 3, stride=2, padding=1),\n",
        "                            nn.BatchNorm2d(in_chans*4),\n",
        "                            nn.ReLU(inplace=True),\n",
        "                            nn.Conv2d(in_chans*4, dim[i], 3, stride=1, padding=1),\n",
        "                            nn.BatchNorm2d(dim[i]),\n",
        "                            nn.ReLU(inplace=True),\n",
        "                        ))\n",
        "            else:\n",
        "                self.stage.append(Stage(img_size=img_size//(2**(i+2)), patch_size=patch_size[i], in_chans=dim[i], dim=dim[i], out_dim=dim[i]*2,\n",
        "                            depth=depth[i], num_heads=num_heads[i], num_inner_head=num_inner_head[i], mlp_ratio=mlp_ratio, qkv_bias=False,\n",
        "                            drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=drop_path_rate, norm_layer=nn.LayerNorm))\n",
        "                self.downsample.append(nn.AvgPool2d(kernel_size=stride[i]))\n",
        "\n",
        "        self.norm = norm_layer(dim[3]*2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.head = nn.Linear(dim[3]*2, num_classes)\n",
        "\n",
        "        trunc_normal_(self.row_pos, std=.02)\n",
        "        trunc_normal_(self.pixel_pos, std=.02)\n",
        "        trunc_normal_(self.column_pos, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight)\n",
        "            if isinstance(m, nn.Conv2d) and m.bias is not None:\n",
        "                trunc_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        img_ds = self.downsample[0](x)\n",
        "        img_merge = self.stage[0](img_ds, self.pixel_pos, self.row_pos, self.column_pos)\n",
        "        for i in range(3):\n",
        "            img_ds = self.downsample[i+1](img_merge)\n",
        "            img_merge = self.stage[i+1](img_ds)\n",
        "\n",
        "        return img_merge\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.forward_features(x)\n",
        "        output_flat = self.avgpool(output).flatten(1)\n",
        "        output_flat = self.norm(output_flat)\n",
        "        output_flat = self.head(output_flat)\n",
        "\n",
        "        return output_flat\n",
        "\n",
        "# ✅ 修改默认分类数为 4\n",
        "\n",
        "def create_model(embed_dim=640, num_classes=4, **kwargs):\n",
        "    model = HoverTrans(embed_dim=embed_dim, num_classes=num_classes, **kwargs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "az-Ct_FuYa3_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.py"
      ],
      "metadata": {
        "id": "pgm6w15wZF8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import importlib\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/PCC2/HoVerTrans-main/HoVerTrans-main')\n",
        "\n",
        "# 强制重新加载 config 模块\n",
        "import config\n",
        "importlib.reload(config)\n",
        "from config import config  # ✅ 此时 config 是函数，不是变量\n"
      ],
      "metadata": {
        "id": "r1v9obucf0od"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import hovertrans\n",
        "importlib.reload(hovertrans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSHTGYk_hmV5",
        "outputId": "4bdb8ae5-e17b-4474-9bce-8083bf78eba4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'hovertrans' from '/content/drive/MyDrive/PCC2/HoVerTrans-main/HoVerTrans-main/hovertrans.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import utils\n",
        "importlib.reload(utils)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR7XyErNjZ61",
        "outputId": "c634f235-b5d4-47f0-dd01-aa13a8998cf0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'utils' from '/content/drive/MyDrive/PCC2/HoVerTrans-main/HoVerTrans-main/utils.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import valid\n",
        "importlib.reload(valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmcsP08tsunW",
        "outputId": "b3cd411f-bbaa-4a0d-e518-70d58c1c79d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'valid' from '/content/drive/MyDrive/PCC2/HoVerTrans-main/HoVerTrans-main/valid.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import utils\n",
        "from config import config\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from valid import valid\n",
        "from hovertrans import create_model\n",
        "from utils import confusion_matrix\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "def train(config, train_loader, test_loader, fold, test_idx):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#MODEL\n",
        "    model = create_model(\n",
        "        img_size=config.img_size,\n",
        "        num_classes=config.class_num,\n",
        "        drop_rate=0.1,\n",
        "        attn_drop_rate=0.1,\n",
        "        patch_size=config.patch_size,\n",
        "        dim=config.dim,\n",
        "        depth=config.depth,\n",
        "        num_heads=config.num_heads,\n",
        "        num_inner_head=config.num_inner_head\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    if config.optimizer == 'Adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "    elif config.optimizer == 'AdamW':\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
        "    elif config.optimizer == 'SGD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    if config.scheduler == 'cosine':\n",
        "        lr_lambda = lambda epoch: (epoch * (1 - config.warmup_decay) / config.warmup_epochs + config.warmup_decay) \\\n",
        "            if epoch < config.warmup_epochs else \\\n",
        "            (1 - config.min_lr / config.lr) * 0.5 * (math.cos((epoch - config.warmup_epochs) /\n",
        "            (config.epochs - config.warmup_epochs) * math.pi) + 1) + config.min_lr / config.lr\n",
        "        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    elif config.scheduler == 'step':\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config.step, gamma=0.9)\n",
        "\n",
        "    writer = SummaryWriter(comment='_' + config.model_name + '_' + config.writer_comment + '_' + str(fold))\n",
        "\n",
        "    print(\"START TRAINING\")\n",
        "    best_acc = 0\n",
        "    ckpt_path = os.path.join(config.model_path, config.model_name, config.writer_comment)\n",
        "    model_save_path = os.path.join(ckpt_path, str(fold))\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        cm = torch.zeros((config.class_num, config.class_num))\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for pack in train_loader:\n",
        "            images = pack['imgs'].to(device)\n",
        "            if images.shape[1] == 1:\n",
        "                images = images.expand((-1, 3, -1, -1))\n",
        "            labels = pack['labels'].to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            pred = output.argmax(dim=1)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            cm = confusion_matrix(pred.detach(), labels.detach(), cm)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        if (epoch + 1) % config.log_step == 0:\n",
        "            print('[epoch %d]' % epoch)\n",
        "            with torch.no_grad():\n",
        "                val_loss, val_acc, sen, spe, auc, pre, f1score = valid(config, model, test_loader, criterion)\n",
        "            writer.add_scalar('Val/F1score', f1score, epoch)\n",
        "            writer.add_scalar('Val/Pre', pre, epoch)\n",
        "            writer.add_scalar('Val/Spe', spe, epoch)\n",
        "            writer.add_scalar('Val/Sen', sen, epoch)\n",
        "            writer.add_scalar('Val/AUC', auc, epoch)\n",
        "            writer.add_scalar('Val/Acc', val_acc, epoch)\n",
        "            writer.add_scalar('Val/Val_loss', val_loss, epoch)\n",
        "\n",
        "            if epoch > config.epochs // 4 and val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                print(\"=> saved best model\")\n",
        "                os.makedirs(model_save_path, exist_ok=True)\n",
        "                if config.save_model:\n",
        "                    torch.save(model.state_dict(), os.path.join(model_save_path, 'bestmodel.pth'))\n",
        "                with open(os.path.join(model_save_path, 'result.txt'), 'w') as f:\n",
        "                    f.write('Best Result:\\n')\n",
        "                    f.write('Acc: %f, Spe: %f, Sen: %f, AUC: %f, Pre: %f, F1score: %f' % (val_acc, spe, sen, auc, pre, f1score))\n",
        "\n",
        "        if epoch + 1 == config.epochs:\n",
        "            with torch.no_grad():\n",
        "                val_loss, val_acc, sen, spe, auc, pre, f1score = valid(config, model, test_loader, criterion)\n",
        "            if config.save_model:\n",
        "                torch.save(model.state_dict(), os.path.join(model_save_path, 'last_epoch_model.pth'))\n",
        "            with open(os.path.join(model_save_path, 'result.txt'), 'a') as f:\n",
        "                f.write('\\nLast Result:\\n')\n",
        "                f.write('Acc: %f, Spe: %f, Sen: %f, AUC: %f, Pre: %f, F1score: %f' % (val_acc, spe, sen, auc, pre, f1score))\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        writer.add_scalar('Train/LR', optimizer.param_groups[0]['lr'], epoch)\n",
        "        writer.add_scalar('Train/Acc', cm.diag().sum() / cm.sum(), epoch)\n",
        "        writer.add_scalar('Train/Avg_epoch_loss', avg_epoch_loss, epoch)\n",
        "\n",
        "def seed_torch(seed=1):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_torch(42)\n",
        "    #加路径\n",
        "    args = config([\n",
        "    '--data_path', '/content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/data',\n",
        "    '--csv_path', '/content/drive/MyDrive/PCC2/have_birads/GDPH_SYSUCC/BIRADS_FOLD_4class.csv'\n",
        "])\n",
        "\n",
        "    cv = KFold(n_splits=args.fold, random_state=42, shuffle=True)\n",
        "    fold = 0\n",
        "\n",
        "    train_set = utils.get_dataset(args.data_path, args.csv_path, args.img_size, mode='train')\n",
        "    test_set = utils.get_dataset(args.data_path, args.csv_path, args.img_size, mode='test')\n",
        "\n",
        "    save_path = os.path.join(args.model_path, args.model_name, args.writer_comment)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    with open(os.path.join(save_path, 'model_info.txt'), 'w') as f:\n",
        "        f.write(str(args))\n",
        "\n",
        "    for train_idx, test_idx in cv.split(train_set):\n",
        "        print(\"\\nCross validation fold %d\" % fold)\n",
        "        train_loader = DataLoader(train_set, batch_size=args.batch_size, sampler=SubsetRandomSampler(train_idx), num_workers=6)\n",
        "        test_loader = DataLoader(test_set, batch_size=1, sampler=SubsetRandomSampler(test_idx))\n",
        "        train(args, train_loader, test_loader, fold, test_idx)\n",
        "        fold += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RscwL4t3ZHK6",
        "outputId": "cad4c3b4-9dd8-4cec-f764-b92e3a78f80c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross validation fold 0\n",
            "START TRAINING\n",
            "[epoch 4]\n",
            "START VALIDING\n",
            "[epoch 9]\n",
            "START VALIDING\n",
            "[epoch 14]\n",
            "START VALIDING\n",
            "[epoch 19]\n",
            "START VALIDING\n",
            "[epoch 24]\n",
            "START VALIDING\n",
            "[epoch 29]\n",
            "START VALIDING\n",
            "[epoch 34]\n",
            "START VALIDING\n",
            "[epoch 39]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 44]\n",
            "START VALIDING\n",
            "[epoch 49]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 54]\n",
            "START VALIDING\n",
            "[epoch 59]\n",
            "START VALIDING\n",
            "[epoch 64]\n",
            "START VALIDING\n",
            "[epoch 69]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 74]\n",
            "START VALIDING\n",
            "[epoch 79]\n",
            "START VALIDING\n",
            "[epoch 84]\n",
            "START VALIDING\n",
            "[epoch 89]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 94]\n",
            "START VALIDING\n",
            "[epoch 99]\n",
            "START VALIDING\n",
            "[epoch 104]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 109]\n",
            "START VALIDING\n",
            "[epoch 114]\n",
            "START VALIDING\n",
            "[epoch 119]\n",
            "START VALIDING\n",
            "[epoch 124]\n",
            "START VALIDING\n",
            "[epoch 129]\n",
            "START VALIDING\n",
            "[epoch 134]\n",
            "START VALIDING\n",
            "[epoch 139]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 144]\n",
            "START VALIDING\n",
            "[epoch 149]\n",
            "START VALIDING\n",
            "START VALIDING\n",
            "\n",
            "Cross validation fold 1\n",
            "START TRAINING\n",
            "[epoch 4]\n",
            "START VALIDING\n",
            "[epoch 9]\n",
            "START VALIDING\n",
            "[epoch 14]\n",
            "START VALIDING\n",
            "[epoch 19]\n",
            "START VALIDING\n",
            "[epoch 24]\n",
            "START VALIDING\n",
            "[epoch 29]\n",
            "START VALIDING\n",
            "[epoch 34]\n",
            "START VALIDING\n",
            "[epoch 39]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 44]\n",
            "START VALIDING\n",
            "[epoch 49]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 54]\n",
            "START VALIDING\n",
            "[epoch 59]\n",
            "START VALIDING\n",
            "[epoch 64]\n",
            "START VALIDING\n",
            "[epoch 69]\n",
            "START VALIDING\n",
            "[epoch 74]\n",
            "START VALIDING\n",
            "[epoch 79]\n",
            "START VALIDING\n",
            "[epoch 84]\n",
            "START VALIDING\n",
            "[epoch 89]\n",
            "START VALIDING\n",
            "[epoch 94]\n",
            "START VALIDING\n",
            "[epoch 99]\n",
            "START VALIDING\n",
            "[epoch 104]\n",
            "START VALIDING\n",
            "[epoch 109]\n",
            "START VALIDING\n",
            "[epoch 114]\n",
            "START VALIDING\n",
            "[epoch 119]\n",
            "START VALIDING\n",
            "[epoch 124]\n",
            "START VALIDING\n",
            "[epoch 129]\n",
            "START VALIDING\n",
            "[epoch 134]\n",
            "START VALIDING\n",
            "[epoch 139]\n",
            "START VALIDING\n",
            "[epoch 144]\n",
            "START VALIDING\n",
            "[epoch 149]\n",
            "START VALIDING\n",
            "START VALIDING\n",
            "\n",
            "Cross validation fold 2\n",
            "START TRAINING\n",
            "[epoch 4]\n",
            "START VALIDING\n",
            "[epoch 9]\n",
            "START VALIDING\n",
            "[epoch 14]\n",
            "START VALIDING\n",
            "[epoch 19]\n",
            "START VALIDING\n",
            "[epoch 24]\n",
            "START VALIDING\n",
            "[epoch 29]\n",
            "START VALIDING\n",
            "[epoch 34]\n",
            "START VALIDING\n",
            "[epoch 39]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 44]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 49]\n",
            "START VALIDING\n",
            "[epoch 54]\n",
            "START VALIDING\n",
            "[epoch 59]\n",
            "START VALIDING\n",
            "[epoch 64]\n",
            "START VALIDING\n",
            "[epoch 69]\n",
            "START VALIDING\n",
            "[epoch 74]\n",
            "START VALIDING\n",
            "[epoch 79]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 84]\n",
            "START VALIDING\n",
            "[epoch 89]\n",
            "START VALIDING\n",
            "[epoch 94]\n",
            "START VALIDING\n",
            "[epoch 99]\n",
            "START VALIDING\n",
            "[epoch 104]\n",
            "START VALIDING\n",
            "[epoch 109]\n",
            "START VALIDING\n",
            "[epoch 114]\n",
            "START VALIDING\n",
            "[epoch 119]\n",
            "START VALIDING\n",
            "[epoch 124]\n",
            "START VALIDING\n",
            "[epoch 129]\n",
            "START VALIDING\n",
            "[epoch 134]\n",
            "START VALIDING\n",
            "[epoch 139]\n",
            "START VALIDING\n",
            "[epoch 144]\n",
            "START VALIDING\n",
            "[epoch 149]\n",
            "START VALIDING\n",
            "START VALIDING\n",
            "\n",
            "Cross validation fold 3\n",
            "START TRAINING\n",
            "[epoch 4]\n",
            "START VALIDING\n",
            "[epoch 9]\n",
            "START VALIDING\n",
            "[epoch 14]\n",
            "START VALIDING\n",
            "[epoch 19]\n",
            "START VALIDING\n",
            "[epoch 24]\n",
            "START VALIDING\n",
            "[epoch 29]\n",
            "START VALIDING\n",
            "[epoch 34]\n",
            "START VALIDING\n",
            "[epoch 39]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 44]\n",
            "START VALIDING\n",
            "[epoch 49]\n",
            "START VALIDING\n",
            "[epoch 54]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 59]\n",
            "START VALIDING\n",
            "[epoch 64]\n",
            "START VALIDING\n",
            "[epoch 69]\n",
            "START VALIDING\n",
            "[epoch 74]\n",
            "START VALIDING\n",
            "[epoch 79]\n",
            "START VALIDING\n",
            "[epoch 84]\n",
            "START VALIDING\n",
            "[epoch 89]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 94]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 99]\n",
            "START VALIDING\n",
            "[epoch 104]\n",
            "START VALIDING\n",
            "[epoch 109]\n",
            "START VALIDING\n",
            "[epoch 114]\n",
            "START VALIDING\n",
            "[epoch 119]\n",
            "START VALIDING\n",
            "[epoch 124]\n",
            "START VALIDING\n",
            "[epoch 129]\n",
            "START VALIDING\n",
            "[epoch 134]\n",
            "START VALIDING\n",
            "[epoch 139]\n",
            "START VALIDING\n",
            "[epoch 144]\n",
            "START VALIDING\n",
            "[epoch 149]\n",
            "START VALIDING\n",
            "START VALIDING\n",
            "\n",
            "Cross validation fold 4\n",
            "START TRAINING\n",
            "[epoch 4]\n",
            "START VALIDING\n",
            "[epoch 9]\n",
            "START VALIDING\n",
            "[epoch 14]\n",
            "START VALIDING\n",
            "[epoch 19]\n",
            "START VALIDING\n",
            "[epoch 24]\n",
            "START VALIDING\n",
            "[epoch 29]\n",
            "START VALIDING\n",
            "[epoch 34]\n",
            "START VALIDING\n",
            "[epoch 39]\n",
            "START VALIDING\n",
            "=> saved best model\n",
            "[epoch 44]\n",
            "START VALIDING\n",
            "[epoch 49]\n",
            "START VALIDING\n",
            "[epoch 54]\n",
            "START VALIDING\n",
            "[epoch 59]\n",
            "START VALIDING\n",
            "[epoch 64]\n",
            "START VALIDING\n",
            "[epoch 69]\n",
            "START VALIDING\n",
            "[epoch 74]\n",
            "START VALIDING\n",
            "[epoch 79]\n",
            "START VALIDING\n",
            "[epoch 84]\n",
            "START VALIDING\n",
            "[epoch 89]\n",
            "START VALIDING\n",
            "[epoch 94]\n",
            "START VALIDING\n",
            "[epoch 99]\n",
            "START VALIDING\n",
            "[epoch 104]\n",
            "START VALIDING\n",
            "[epoch 109]\n",
            "START VALIDING\n",
            "[epoch 114]\n",
            "START VALIDING\n",
            "[epoch 119]\n",
            "START VALIDING\n",
            "[epoch 124]\n",
            "START VALIDING\n",
            "[epoch 129]\n",
            "START VALIDING\n",
            "[epoch 134]\n",
            "START VALIDING\n",
            "[epoch 139]\n",
            "START VALIDING\n",
            "[epoch 144]\n",
            "START VALIDING\n",
            "[epoch 149]\n",
            "START VALIDING\n",
            "START VALIDING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "把模型数据存到本地"
      ],
      "metadata": {
        "id": "rQhmO4KMlT2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import shutil for file operations\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Step 3: Define source and target paths\n",
        "source_path = '/content/weight/hovertrans/BIRADS4CLASS'\n",
        "target_path = '/content/drive/MyDrive/HoVerTrans_Results/BIRADS4CLASS'\n",
        "\n",
        "# Step 4: Copy the entire folder to Google Drive\n",
        "if os.path.exists(target_path):\n",
        "    print(\"Target already exists. Overwriting...\")\n",
        "    shutil.rmtree(target_path)\n",
        "\n",
        "shutil.copytree(source_path, target_path)\n",
        "print(\"✅ Model files successfully saved to your Google Drive!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ANoK9-lS5V",
        "outputId": "17733f77-abae-4eb2-9b6e-9f540138bc50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Model files successfully saved to your Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "valid.py"
      ],
      "metadata": {
        "id": "DK7KaetQmWXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from utils import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/PCC2/HoVerTrans-main/HoVerTrans-main')\n",
        "\n",
        "def valid(config, net, val_loader, criterion):\n",
        "    device = next(net.parameters()).device\n",
        "    net.eval()\n",
        "\n",
        "    print(\"START VALIDING\")\n",
        "    epoch_loss = 0\n",
        "    y_true, y_score = [], []\n",
        "\n",
        "    cm = torch.zeros((config.class_num, config.class_num))\n",
        "    for i, pack in enumerate(val_loader):\n",
        "        images = pack['imgs'].to(device)\n",
        "        if images.shape[1] == 1:\n",
        "            images = images.expand((-1, 3, -1, -1))\n",
        "        names = pack['names']\n",
        "        labels = pack['labels'].to(device)\n",
        "\n",
        "        output = net(images)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "        y_true.append(labels.detach().cpu().item())\n",
        "\n",
        "        # 使用预测为该样本属于标签1的概率；可根据需要调整 index\n",
        "        y_score.append(output.softmax(dim=1).detach().cpu().numpy()[0][1])\n",
        "\n",
        "        cm = confusion_matrix(pred.detach(), labels.detach(), cm)\n",
        "        epoch_loss += loss.detach().cpu()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(val_loader)\n",
        "\n",
        "    acc = cm.diag().sum() / cm.sum()\n",
        "\n",
        "    # 宏平均 Sensitivity（Recall）和 Specificity\n",
        "    sen_per_class = cm.diag() / (cm.sum(dim=1) + 1e-6)\n",
        "    spe_per_class = cm.diag() / (cm.sum(dim=0) + 1e-6)\n",
        "\n",
        "    sen = sen_per_class.mean().item()\n",
        "    spe = spe_per_class.mean().item()\n",
        "    pre = cm.diag().sum() / (cm.sum(dim=1).sum() + 1e-6)  # 宏平均 Precision\n",
        "    rec = sen\n",
        "    f1score = 2 * pre * rec / (pre + rec + 1e-6)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_score, multi_class='ovr')  # one-vs-rest AUC\n",
        "    except:\n",
        "        auc = 0.0  # 若某一类没有样本，防止 AUC 报错\n",
        "\n",
        "    return [avg_epoch_loss, acc.item(), sen, spe, auc, pre, f1score]\n"
      ],
      "metadata": {
        "id": "EDw-iUUNmX2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}