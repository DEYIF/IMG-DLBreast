# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OxMYm8NjFZVjxro7Fxm4MgO_qZmnbQmR

Train
"""

# %%

import matplotlib
matplotlib.use('Agg')  # headless plotting
import matplotlib.pyplot as plt
import torch
# from models import BaselineResNet, BayesianResNet
import torchvision.models as models
from torch.utils.data import DataLoader,random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau
import argparse
import tqdm
import numpy as np
from google.colab import drive
import sys

#add path
sys.path.append('/content/drive/MyDrive')

#import package above
from dataImport import DBTData
from utils import accuracy

!pip install tensorboardX
from tensorboardX import SummaryWriter
import os

import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

# enable cuda if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# setup tensorboardx
writer = SummaryWriter()

# dimension properties
batch_size = 10
val_batch_size = batch_size
num_workers = 0
num_classes = 2
lambda_prop = 0.01

# load data
# load data
color = True
resize_to = (256, 256)
dataset_train = DBTData('train_phase2','train',crop_to=(224,224), resize_to=resize_to, color=color)
dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)
dataset_valid = DBTData('train_phase2','valid',crop_to=(512,512), resize_to=resize_to, color=color)
dataloader_valid = DataLoader(dataset_valid, batch_size=val_batch_size, num_workers=num_workers)

assert len(dataset_train) > 0
assert len(dataset_valid) > 0
print("Train dataset length:", len(dataset_train))
print("Valid dataset length:", len(dataset_valid))
print('')

# create a model
model = torch.nn.Module()
model = models.resnet50(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = torch.nn.Linear(num_ftrs, num_classes)
model = model.to(device)

# create your optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4,betas=(0.9, 0.999),weight_decay=1e-8)
lr_scheduler = ReduceLROnPlateau(optimizer, patience=5)

# create loss function
criterion = torch.nn.CrossEntropyLoss()

print('')  # print empty line before training output

# save accuracies and losses during training
train_losses = []
train_accuracies = []
valid_losses = []
valid_accuracies = []

start_epoch = 0
epochs = 100
e = 0
batch_counter = 0
batch_counter_valid = 0

for e in range(start_epoch, epochs):

    # go through training set
    model.train()
    print("lr =", optimizer.param_groups[0]['lr'])

    epoch_train_loss = []
    epoch_train_acc = []
    is_best = False

    x, y, y_pred = None, None, None
    batches = tqdm.tqdm(dataloader_train)
    for x, y in batches:
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()

        y_pred = model(x)
        train_loss = criterion(y_pred, y)

        train_loss.backward()
        optimizer.step()

        # print current loss
        batches.set_description("loss: {:4f}".format(train_loss.item()))

        # sum epoch loss
        epoch_train_loss.append(train_loss.item())

        # calculate batch train accuracy
        batch_acc = accuracy(y_pred, y)
        epoch_train_acc.append(batch_acc)

        writer.add_scalar('data/train_loss', train_loss.item(), batch_counter)
        writer.add_scalar('data/train_acc', batch_acc, batch_counter)
        batch_counter += 1

    epoch_train_loss = np.mean(epoch_train_loss)
    epoch_train_acc = np.mean(epoch_train_acc)
    lr_scheduler.step(epoch_train_loss)

    # go through validation set
    model.eval()
    with torch.no_grad():

        epoch_valid_loss = []
        epoch_valid_acc = []

        batches = tqdm.tqdm(dataloader_valid)
        for x, y in batches:
            x, y = x.to(device), y.to(device)

            y_pred = model(x)
            valid_loss = criterion(y_pred, y)

            # print current loss
            batches.set_description("loss: {:4f}".format(valid_loss.item()))

            # sum epoch loss
            epoch_valid_loss.append(valid_loss.item())

            # calculate batch train accuracy
            batch_acc = accuracy(y_pred, y)
            epoch_valid_acc.append(batch_acc)

            writer.add_scalar('data/valid_classifier_loss', valid_loss.item(), batch_counter_valid)
            writer.add_scalar('data/valid_acc', batch_acc, batch_counter_valid)
            batch_counter_valid += 1

    epoch_valid_loss = np.mean(epoch_valid_loss)
    epoch_valid_acc = np.mean(epoch_valid_acc)

    print("Epoch {:d}: loss: {:4f}, acc: {:4f}, val_loss: {:4f}, val_acc: {:4f}"
          .format(e,
                  epoch_train_loss,
                  epoch_train_acc,
                  epoch_valid_loss,
                  epoch_valid_acc,
                  ))

    # save epoch losses
    train_losses.append(epoch_train_loss)
    train_accuracies.append(epoch_train_acc)
    valid_losses.append(epoch_valid_loss)
    valid_accuracies.append(epoch_valid_acc)

    if valid_losses[-1] <= np.min(valid_losses):
        is_best = True

    if is_best:
        filename = "/content/drive/MyDrive/snapshots" + "_best.pth.tar"  
        print("Saving best weights so far with val_loss: {:4f}".format(valid_losses[-1]))
        torch.save({
            'epoch': e,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'train_losses': train_losses,
            'train_accs': train_accuracies,
            'val_losses': valid_losses,
            'val_accs': valid_accuracies,
        }, filename)

    if e == epochs-1:
      #define Google Drive path folder 
        import os
        from google.colab import drive

        #Google Drive
        drive.mount('/content/drive')

        #define Google Drive path folder 
        filename = "/content/drive/MyDrive/trainmodels" + "_" + str(e) + ".pth.tar"
        print("Saving weights at epoch {:d}".format(e))
        torch.save({
            'epoch': e,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'train_losses': train_losses,
            'train_accs': train_accuracies,
            'val_losses': valid_losses,
            'val_accs': valid_accuracies,
            }, filename)

    print('')

    # plot losses
    plt.figure()
    plt.plot(range(len(train_losses)), train_losses, marker='x')
    plt.plot(range(len(valid_losses)), valid_losses, marker='x')
    plt.title("loss")
    plt.xlabel("epoch")
    plt.ylabel("loss")

    plt.savefig("_loss.pdf", dpi=300)
    plt.figure()
    plt.plot(range(len(train_accuracies)), train_accuracies, marker='x')
    plt.plot(range(len(valid_accuracies)), valid_accuracies, marker='x')
    plt.title("accuracy")
    plt.xlabel("epoch")
    plt.ylabel("acc")
    plt.savefig("acc.pdf", dpi=300)
    plt.close('all')